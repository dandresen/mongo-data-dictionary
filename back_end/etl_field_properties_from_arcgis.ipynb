{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.gis import GIS\n",
    "from arcgis.features import FeatureLayer, FeatureLayerCollection\n",
    "import pandas as pd\n",
    "import urllib3\n",
    "import pymongo\n",
    "\n",
    "\n",
    "urllib3.disable_warnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please sign in to your GIS and paste the code that is obtained below.\n",
      "If a web browser does not automatically open, please navigate to the URL below yourself instead.\n",
      "Opening web browser to navigate to: https://txdot.maps.arcgis.com/sharing/rest/oauth2/authorize?response_type=code&client_id=etCLdRS2UQTqx7zx&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&state=s7vN1pAkORFrmExdd82w1fbiuHpp58\n",
      "Enter code obtained on signing in using SAML: ········\n",
      "Successfully logged in as: TXDOT_GIS\n"
     ]
    }
   ],
   "source": [
    "gis = GIS(\"https://txdot.maps.arcgis.com\", client_id='etCLdRS2UQTqx7zx')\n",
    "print(\"Successfully logged in as: \" + gis.properties.user.username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These scripts are querying the ArcGIS API, by owner and item type. They further\n",
    "query by authroitative status and if the data are available to the public.\n",
    "Lastly, the dictionary that is returned by the API is converted into a list of\n",
    "directories that shows only the information that is required. In this case, the field properties.\n",
    "\n",
    "\"\"\"\n",
    "# search content by owner and type\n",
    "items = gis.content.search(query=\"owner:TPP_GIS\", \n",
    "                           item_type='Feature *',\n",
    "                           max_items=200) # for testing \n",
    "\n",
    "# query authoritative layers from content search\n",
    "item_URL = [item.url for item in items \n",
    "            if (item.content_status=='org_authoritative' \n",
    "                and item.access=='public' \n",
    "                and item.url !=None)]\n",
    "\n",
    "# create FeatureLayerCollection- need to access the data not just the item\n",
    "layer_URL = [FeatureLayerCollection(lyr).layers for lyr in item_URL]\n",
    "\n",
    "\n",
    "# create a list of dict. items of the properties \n",
    "lyrNames = []\n",
    "for lyr in layer_URL:\n",
    "    for lyrname in lyr:\n",
    "       lyrNames.append(lyrname.properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see how many layers you have\n",
    "len(lyrNames) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here is a bunch of Pandas stuff. Bascially I took a list of dictonary layer items, with many fields returned by the API\n",
    "and then converted it so the fields show what layers they are in. I also included the item_id which can be used \n",
    "for links to the data on AGO and ODP. The result is a dictonary with the field as the key and the items (including \n",
    "arrays of item_id, layer, applications) as the values. **Note that applications comes from a local csv, not the API\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# create a dataframe showing the dict. data from the layer properties \n",
    "df_rows = []\n",
    "for data in lyrNames:\n",
    "    data_row = data.fields\n",
    "    lyr_name = data.name\n",
    "    item_id = data.serviceItemId\n",
    "      \n",
    "    for row in data_row:\n",
    "        row['layer'] = lyr_name\n",
    "        row['item_id'] = item_id\n",
    "        df_rows.append(row)\n",
    "        \n",
    "df = pd.DataFrame(df_rows)\n",
    "\n",
    "# groupby field name and concatincate all layers into one field \n",
    "# then, drop the duplicates, which leaves one row showing all of the\n",
    "# field information \n",
    "df['layer'] =  df.groupby(['name'])['layer'].transform(lambda x : ','.join(x))\n",
    "df['item_id'] = df.groupby(['name'])['item_id'].transform(lambda x : ','.join(x))\n",
    "df = df.drop_duplicates(['name','layer'])\n",
    "\n",
    "# at this point, add the names of all of the applications from a csv\n",
    "# note that this comes in as a true list (['app1', 'app2']) so no need to split (see below)\n",
    "app_list = pd.DataFrame(pd.read_csv('data/applications.csv'))['app_name'].tolist()\n",
    "df['all_applications'] = [app_list for i in df.index]\n",
    "\n",
    "# the above code creates [foo, bar, awesome],importing into mongo did not \n",
    "# like that format, so changed it to a list \n",
    "\n",
    "# get the series and turn it into a useable list ['foo', 'bar', 'awesome']\n",
    "df['layer'] = df['layer'].str.split(',')\n",
    "df['item_id'] = df['item_id'].str.split(',')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Create a dictionary and then replace pandas df index key with the field name\n",
    "this can be used later to cross check fields of layers\n",
    "\n",
    "NOTE that you can't use df.set_index('name') b/c it drops the name key, which\n",
    "is important to have for queries, cross checks, etc. Therefore, replacing it\n",
    "via a dict. compensastaion works nicely\n",
    "\n",
    "\"\"\"\n",
    "df_dict_wrong_id = df.to_dict('index')\n",
    "df_dict = {str(k).replace(str(k), v['name']) : v for (k,v) in df_dict_wrong_id.items()}\n",
    "\n",
    "# df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layers going in is 82 and unique layers in the dataframe is 82\n"
     ]
    }
   ],
   "source": [
    "# a test to see if the amount of layers in objectId is the same as the \n",
    "# layers going in. The iloc[0] assumes that objectId is at the first location\n",
    "# of the list\n",
    "\n",
    "test_objId = df.iloc[0]['layer']\n",
    "if len(test_objId) != len(layer_URL):\n",
    "    print(f'HEY!!! SOMETHING BROKE! Layers going in is {len(layer_URL)} and unique layers in the dataframe is {len(test_objId)}')\n",
    "else:\n",
    "    print(f'Layers going in is {len(layer_URL)} and unique layers in the dataframe is {len(test_objId)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DANDR1-C\\Miniconda3\\envs\\data-dict-test\\lib\\site-packages\\pymongo\\compression_support.py:55: UserWarning: Unsupported compressor: disabled\n",
      "  warnings.warn(\"Unsupported compressor: %s\" % (compressor,))\n",
      "C:\\Users\\DANDR1-C\\Miniconda3\\envs\\data-dict-test\\lib\\site-packages\\pymongo\\common.py:781: UserWarning: Unknown option gssapiservicename\n",
      "  warnings.warn(str(exc))\n"
     ]
    }
   ],
   "source": [
    "# connect to the local mongo db\n",
    "client = pymongo.MongoClient(\"mongodb://127.0.0.1:27017/?compressors=disabled&gssapiServiceName=mongodb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped and created test_esri_api_collection!\n",
      "loaded data\n"
     ]
    }
   ],
   "source": [
    "# load data into mongo\n",
    "# gonig to iterate over the dict by using the keys and then load_one to mongo\n",
    "# this won't be the most elegant, but the load_many option was not agreeing with my dict\n",
    "\n",
    "db = client.testdb\n",
    "\n",
    "collection = db.test_esri_api_collection\n",
    "\n",
    "# leaving this drop here for testing\n",
    "if 'test_esri_api_collection' in db.list_collection_names():\n",
    "    collection.drop()\n",
    "    collection = db.test_esri_api_collection\n",
    "    print('Dropped and created test_esri_api_collection!')\n",
    "\n",
    "\n",
    "# load the dictionary into mongo, couldn't get insert_many to place nice, \n",
    "# so wrote insert_one into a for loop\n",
    "\n",
    "# using the mongo assigned _id objectid \n",
    "list_keys = list(df_dict.keys())\n",
    "for lyr in list_keys:\n",
    "    df_dict_load = df_dict[str(lyr)]\n",
    "    mongo_results = collection.insert_one(df_dict_load)\n",
    "#     print(f\"Loaded document: {df_dict[lyr]['name']}\", end=\" \")\n",
    "print('loaded data')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
